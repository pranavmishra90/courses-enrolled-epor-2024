[
  {
    "objectID": "lectures/07 - Regression and Correlation.html",
    "href": "lectures/07 - Regression and Correlation.html",
    "title": "Week 7 - Regression and Correlation",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display, Markdown, Math, Latex\n\nfrom scipy import stats\nimport math\n\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\nfrom statsmodels.stats.anova import AnovaRM\nfrom statsmodels.regression.mixed_linear_model import MixedLMResults\n\nimport pingouin as pg",
    "crumbs": [
      "Lectures",
      "Week 7 - Regression and Correlation"
    ]
  },
  {
    "objectID": "lectures/07 - Regression and Correlation.html#estimating-alpha-and-beta-by-the-least-squares-criterion",
    "href": "lectures/07 - Regression and Correlation.html#estimating-alpha-and-beta-by-the-least-squares-criterion",
    "title": "Week 7 - Regression and Correlation",
    "section": "Estimating \\(\\alpha\\) and \\(\\beta\\) by the least squares criterion",
    "text": "Estimating \\(\\alpha\\) and \\(\\beta\\) by the least squares criterion\nWe want to chose an a and b to minimize:\n\\[\n\\sum_{i=1}^{n} [y_i - (a + bx_i)]^{2}\n\\]\n\nNormal equations\n(Not the normal distributions, but just a mathematical name).\n$ y_i = na + bx_i $\n$ x_i y_i = a x_i + b ^2 $\nResidual: $ e_i = y_i - = y_i - (a + bx_i) $\n$ s_{y,x} = _ {y,x} $, where \\(s_{y,x}\\) is an estimate of \\(\\sigma _ {y,x}\\)\n\n\nTest and Confidence Intervals\nUnder the null hypothesis, \\(\\beta = 0\\)\n$ t = $ has a \\(t\\) distribution on \\(n-2\\) degrees of freedom\nA \\(1-\\alpha\\) confidence interval for \\(\\beta\\) is given by:\n\\[\nb \\pm = t_\\alpha s_b\n\\]\nIntercept:\n$ t = $ and $ a t_s_a $\n\nage_kidney_json  = {\n    2: [20],\n    3: [18],\n    4: [22, 25],\n    5: [17, 20, 20, 22],\n    6: [21, 22],\n    7: [20, 20, 22, 24],\n    8: [18, 25, 33],\n    9: [27, 31],\n    10: [18, 24, 34],\n    11: [25, 28]\n}\n\nage_kidney_df = pd.DataFrame([(age, length) for age, lengths in age_kidney_json.items() for length in lengths],\n                  columns=[\"age\", \"length\"])\n\n# age_kidney_df\n\n\nX = age_kidney_df['age']\ny = age_kidney_df['length']\n\n\n# Add a constant term to the predictor variable (age)\nX = sm.add_constant(X)\n\n\n# Fit the linear regression model\nmodel = sm.OLS(y, X).fit()\n\nmodel.summary()\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nlength\nR-squared:\n0.269\n\n\nModel:\nOLS\nAdj. R-squared:\n0.236\n\n\nMethod:\nLeast Squares\nF-statistic:\n8.113\n\n\nDate:\nThu, 23 May 2024\nProb (F-statistic):\n0.00935\n\n\nTime:\n16:28:11\nLog-Likelihood:\n-66.806\n\n\nNo. Observations:\n24\nAIC:\n137.6\n\n\nDf Residuals:\n22\nBIC:\n140.0\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n16.4826\n2.491\n6.618\n0.000\n11.317\n21.648\n\n\nage\n0.9606\n0.337\n2.848\n0.009\n0.261\n1.660\n\n\n\n\n\n\nOmnibus:\n1.516\nDurbin-Watson:\n2.194\n\n\nProb(Omnibus):\n0.469\nJarque-Bera (JB):\n0.667\n\n\nSkew:\n0.396\nProb(JB):\n0.716\n\n\nKurtosis:\n3.199\nCond. No.\n22.4\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nintercept, slope = model.params['const'], model.params['age']\n\nprint(f\"Intercept (b0): {intercept:.2f}\")\nprint(f\"Slope (b1): {slope:.2f}\")\n\nIntercept (b0): 16.48\nSlope (b1): 0.96\n\n\n\nconfidence_intervals = model.conf_int()\nconfidence_intervals\n\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\nconst\n11.317180\n21.647946\n\n\nage\n0.261169\n1.660010\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 6))\nsm.graphics.plot_regress_exog(model, 'age', fig=fig)\nplt.show()\n\n\n\n\n\n\n\n\nThe residuals appear to be increasing as the age increases\n\nfig1, ax1 = plt.subplots()\nax1 = sns.scatterplot(data=age_kidney_df, x='age', y='length', ax=ax1)\n\n\n\n\n\n\n\n\n\n\nfig1, ax1 = plt.subplots()\nsns.regplot(data=age_kidney_df, x='age', y='length', ax=ax1,  marker='o')\n\n\n\n\n\n\n\n\n$$ r = \n{}\n$$",
    "crumbs": [
      "Lectures",
      "Week 7 - Regression and Correlation"
    ]
  },
  {
    "objectID": "lectures/07 - Regression and Correlation.html#retinal-permeability",
    "href": "lectures/07 - Regression and Correlation.html#retinal-permeability",
    "title": "Week 7 - Regression and Correlation",
    "section": "Retinal Permeability",
    "text": "Retinal Permeability\n\nretinal_perm_json = {\n    19.5: 0.0,\n    15.0: 38.0,\n    13.5: 59.0,\n    23.3: 119.2,\n    6.3: 97.4,\n    2.0: 129.5,\n    1.8: 1987.0,\n    1.8: 2487.0,\n    1.8: 3180.0,\n    1.8: 4385.0\n}\n\nretinal_perm_df = pd.DataFrame(retinal_perm_json.items(), columns=[\"Penetration\", \"ERG\"])\nretinal_perm_df\n\n\n\n\n\n\n\n\n\nPenetration\nERG\n\n\n\n\n0\n19.5\n0.0\n\n\n1\n15.0\n38.0\n\n\n2\n13.5\n59.0\n\n\n3\n23.3\n119.2\n\n\n4\n6.3\n97.4\n\n\n5\n2.0\n129.5\n\n\n6\n1.8\n4385.0",
    "crumbs": [
      "Lectures",
      "Week 7 - Regression and Correlation"
    ]
  },
  {
    "objectID": "lectures/07 - Regression and Correlation.html#homework",
    "href": "lectures/07 - Regression and Correlation.html#homework",
    "title": "Week 7 - Regression and Correlation",
    "section": "Homework",
    "text": "Homework\n\n8-11\n\nAssociation between high blood pressure, diabetes, and high levels of lipids in the blood\nHTN –&gt; Lower insulin sensitivity vs. normotensive\nPhysicial fitness affects insulin sensitivity\n\nEndre T, Mattiasson I, Hulthén UL, Lindgärde F, Berglund G. Insulin resistance is coupled to low physical fitness in normotensive men with a family history of hypertension. J Hypertens. 1994;12(1):81-88.\n1\n\n# Import raw data from Endre et al\nobesity_df = pd.read_csv('/home/pranav/work/pranavmishra90/courses/enrolled/epor-2024/src/8-16.csv')\n\n# Split into two dataframes for \"Controls\" and \"Relatives\"\nob_controls = obesity_df[obesity_df['Group'] == 'Control']\nob_relatives = obesity_df[obesity_df['Group'] == 'Relative']\n\n\n# Plot the data\n\n# Create a figure with two subplots\nfig, ax = plt.subplots(1, 2, figsize=(12, 6))\n\nx_var = 'Waist_Hip_Ratio'\ny_var = 'LogIS'\n\n# Plot Data\nsns.regplot(x=x_var, y=y_var, data=ob_controls, marker='o', ax=ax[0])\nsns.regplot(x=x_var, y=y_var, data=ob_relatives, marker='o', ax=ax[1])\n\n\n# Set the y-axis limits to be the same\nmax_y = max(ob_controls[y_var].max(), ob_relatives[y_var].max())\nmin_y = min(ob_controls[y_var].min(), ob_relatives[y_var].min())\n\nmax_y_adj = max_y * 1.2\nmin_y_adj = min_y * 0.8\n\nfor ax_instance in ax:\n    ax_instance.set_ylim(min_y_adj, max_y_adj)\n    ax_instance.set_xlim(0.75, 1.15)\n\n\n\n# Formatting\nax[0].set_title('Controls')\nax[1].set_title('Relatives')\n\nText(0.5, 1.0, 'Relatives')\n\n\n\n\n\n\n\n\n\n\n# Controls\n\nX_control = ob_controls[x_var]\ny_control = ob_controls[y_var]\n\n# Add a constant term to the predictor variable\nX_control = sm.add_constant(X_control)\n\n# Fit the linear regression model\nmodel_cont = sm.OLS(y_control, X_control).fit()\n\ndisplay(Markdown(f\"### `Controls`\"))\ndisplay(model_cont.summary())\n\nintercept_cont, slope_cont = model_cont.params['const'], model_cont.params[x_var]\n\nprint(f\"Intercept (b0): {intercept_cont:.2f}\")\nprint(f\"Slope (b1): {slope_cont:.2f}\")\n\nci_control = model_cont.conf_int()\ndisplay(Markdown(f\"#### Confidence Intervals\"))\ndisplay(ci_control)\n\n\nfig, ax = plt.subplots(figsize=(10, 6))\nsm.graphics.plot_regress_exog(model_cont, x_var, fig=fig)\nplt.show()\n\nControls\n\n\n/home/pranav/miniforge3/envs/outcomes/lib/python3.11/site-packages/scipy/stats/_axis_nan_policy.py:531: UserWarning: kurtosistest only valid for n&gt;=20 ... continuing anyway, n=15\n  res = hypotest_fun_out(*samples, **kwds)\n\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nLogIS\nR-squared:\n0.638\n\n\nModel:\nOLS\nAdj. R-squared:\n0.611\n\n\nMethod:\nLeast Squares\nF-statistic:\n22.95\n\n\nDate:\nThu, 23 May 2024\nProb (F-statistic):\n0.000353\n\n\nTime:\n17:06:18\nLog-Likelihood:\n11.061\n\n\nNo. Observations:\n15\nAIC:\n-18.12\n\n\nDf Residuals:\n13\nBIC:\n-16.71\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n2.5793\n0.335\n7.695\n0.000\n1.855\n3.303\n\n\nWaist_Hip_Ratio\n-1.7648\n0.368\n-4.790\n0.000\n-2.561\n-0.969\n\n\n\n\n\n\nOmnibus:\n1.016\nDurbin-Watson:\n1.806\n\n\nProb(Omnibus):\n0.602\nJarque-Bera (JB):\n0.830\n\n\nSkew:\n-0.314\nProb(JB):\n0.660\n\n\nKurtosis:\n2.034\nCond. No.\n20.9\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nIntercept (b0): 2.58\nSlope (b1): -1.76\n\n\n\n\n\n\n\n\n\n\n# Relatives\n\nX_relative = ob_relatives[x_var]\ny_relative = ob_relatives[y_var]\n\n# Add a constant term to the predictor variable\nX_relative = sm.add_constant(X_relative)\n\n# Fit the linear regression model\nmodel_rel = sm.OLS(y_relative, X_relative).fit()\n\ndisplay(Markdown(f\"### `Relatives`\"))\ndisplay(model_rel.summary())\n\nintercept_rel, slope_rel = model_rel.params['const'], model_rel.params[x_var]\n\nprint(f\"Intercept (b0): {intercept_rel:.2f}\")\nprint(f\"Slope (b1): {slope_rel:.2f}\")\n\nci_rel = model_rel.conf_int()\ndisplay(Markdown(f\"#### Confidence Intervals\"))\ndisplay(ci_rel)\n\nfig, ax = plt.subplots(figsize=(10, 6))\nsm.graphics.plot_regress_exog(model_rel, x_var, fig=fig)\nplt.show()\n\nRelatives\n\n\n/home/pranav/miniforge3/envs/outcomes/lib/python3.11/site-packages/scipy/stats/_axis_nan_policy.py:531: UserWarning: kurtosistest only valid for n&gt;=20 ... continuing anyway, n=15\n  res = hypotest_fun_out(*samples, **kwds)\n\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nLogIS\nR-squared:\n0.005\n\n\nModel:\nOLS\nAdj. R-squared:\n-0.072\n\n\nMethod:\nLeast Squares\nF-statistic:\n0.06050\n\n\nDate:\nThu, 23 May 2024\nProb (F-statistic):\n0.810\n\n\nTime:\n17:09:13\nLog-Likelihood:\n2.7763\n\n\nNo. Observations:\n15\nAIC:\n-1.553\n\n\nDf Residuals:\n13\nBIC:\n-0.1364\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n0.6223\n0.604\n1.031\n0.321\n-0.682\n1.926\n\n\nWaist_Hip_Ratio\n0.1611\n0.655\n0.246\n0.810\n-1.254\n1.577\n\n\n\n\n\n\nOmnibus:\n1.661\nDurbin-Watson:\n1.469\n\n\nProb(Omnibus):\n0.436\nJarque-Bera (JB):\n1.325\n\n\nSkew:\n-0.636\nProb(JB):\n0.516\n\n\nKurtosis:\n2.291\nCond. No.\n21.7\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nIntercept (b0): 0.62\nSlope (b1): 0.16\n\n\nConfidence Intervals\n\n\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\nconst\n-0.681670\n1.926291\n\n\nWaist_Hip_Ratio\n-1.254265\n1.576554\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmd_text = f\"\"\"\n\n### Answer:\n\nNo, the relationship between the waist-hip-ratio (WHR) and Insulin Sensitivity are not the same in subjects who have no immediate family member with HTN vs. those who do.\n\nIn the control group, log of Insulin Sensitivity (LogIS) decreased as the WHR increased (slope `{slope_cont:.2f}`). In subjects with HTN relatives, LogIS remained relatively constant as WHR increased (slope `{slope_rel:.2f}`). Furthermore, in the control group, the insulin sensitivity remained high at lower WHR (approx. `1.2` vs `0.8` at WHR of 0.8). \n\nThis suggests that when a person has a first-degree relative with hypertension, their insulin sensitivity is not dependent on WHR and is lower across all WHR, when compared with healthy controls.\n\n\n\"\"\"\n\ndisplay(Markdown(md_text))\n\nAnswer:\nNo, the relationship between the waist-hip-ratio (WHR) and Insulin Sensitivity are not the same in subjects who have no immediate family member with HTN vs. those who do.\nIn the control group, log of Insulin Sensitivity (LogIS) decreased as the WHR increased (slope -1.76). In subjects with HTN relatives, LogIS remained relatively constant as WHR increased (slope 0.16). Furthermore, in the control group, the insulin sensitivity remained high at lower WHR (approx. 1.2 vs 0.8 at WHR of 0.8).\nThis suggests that when a person has a first-degree relative with hypertension, their insulin sensitivity is not dependent on WHR and is lower across all WHR, when compared with healthy controls.",
    "crumbs": [
      "Lectures",
      "Week 7 - Regression and Correlation"
    ]
  },
  {
    "objectID": "lectures/01 - Introduction and Data Summarization.html",
    "href": "lectures/01 - Introduction and Data Summarization.html",
    "title": "Week 1 - Introduction and Data Summarization",
    "section": "",
    "text": "import pandas as pd import numpy as np\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns \nfrom IPython.display import display, Markdown, Math, Latex\n\n\n\nThese problems have been taken from the following textbook, which is used in the course:\nGlantz’s “Primer of Biostatistics”1\n\n\n\n# Import data\nrna_levels_seroconversion = [79725, 12862, 18022, 76712, 256440, 14013, 46083, 6808, 85781, 1251, 6081, 50397, 11020, 13633, 1064, 496433, 25308, 6616, 11210, 13900]\n\nrna_levels_sc_df = pd.DataFrame(rna_levels_seroconversion, columns=['RNA_Level'])\n\n\n# Calculate the descriptive statistics \nrna_levels_sc_descriptive = pd.DataFrame(rna_levels_sc_df.describe().round(), dtype=int)\nrna_levels_sc_descriptive\n\n\n\n\n\n\n\n\n\nRNA_Level\n\n\n\n\ncount\n20\n\n\nmean\n61668\n\n\nstd\n117539\n\n\nmin\n1064\n\n\n25%\n9967\n\n\n50%\n13956\n\n\n75%\n56976\n\n\nmax\n496433\n\n\n\n\n\n\n\n\n\n# Determining normality from descriptive stats\n\nsc_mean = rna_levels_sc_descriptive.loc[\"mean\", \"RNA_Level\"]\nsc_median = rna_levels_sc_descriptive.loc[\"50%\", \"RNA_Level\"]\n\nmean_to_25th = sc_mean - rna_levels_sc_descriptive.loc[\"25%\", \"RNA_Level\"]\nmean_to_75th = sc_mean - rna_levels_sc_descriptive.loc[\"75%\", \"RNA_Level\"]\n\n\n\nFrom the descriptive statistics, the data does not appear to be normally distributed. First, there is some skew to the data, as the mean (61668) is far off from the median (13956).\nAdditionally, the difference between the mean and 25th percentile \\(\\neq\\) difference between the mean and 75th percentile:\n\\[\n\\bar{x} - 25^{th}\\ percentile = 51701\n\\]\n\\[\n\\bar{x} - 75^{th}\\ percentile = 4692\n\\]\nTherefore, we can say that the data appears to be not normally distributed, based on the descriptive statistics alone.\n\n\n\n# Graphical confirmation\nsns.histplot(data = rna_levels_seroconversion)\n\n\n\n\n\n\n\n\n\n\n\n\n# Import Data\nnormalized_data = [4.90, 4.11, 4.26, 4.88, 5.41, 4.15, 4.66, 3.83, 4.93, 3.10, 3.78, 4.70, 4.04, 4.13, 3.03, 5.70, 4.40, 3.82, 4.05, 4.14]\n\nnormalized_data_df = pd.DataFrame(normalized_data, columns=['Pain_Score'])\n\n\n# Calculate the descriptive statistics \nnorm_data_descriptive = pd.DataFrame(normalized_data_df.describe().round(), dtype=int)\nnorm_data_descriptive\n\n\n\n\n\n\n\n\n\nPain_Score\n\n\n\n\ncount\n20\n\n\nmean\n4\n\n\nstd\n1\n\n\nmin\n3\n\n\n25%\n4\n\n\n50%\n4\n\n\n75%\n5\n\n\nmax\n6\n\n\n\n\n\n\n\n\n\n# Determining normality from descriptive stats\n\nnorm_mean = norm_data_descriptive.loc[\"mean\", \"Pain_Score\"]\nnorm_median = norm_data_descriptive.loc[\"50%\", \"Pain_Score\"]\n\nmean_to_25th = norm_mean - norm_data_descriptive.loc[\"25%\", \"Pain_Score\"]\nmean_to_75th = norm_data_descriptive.loc[\"75%\", \"Pain_Score\"] - norm_mean\n\n\n\nFrom the descriptive statistics, the data does appear to be normally distributed. First, the mean (4) \\(=\\) the median (4).\nAdditionally, the difference between the mean and 25th percentile \\(\\approx\\) difference between the mean and 75th percentile:\n\\[\n\\bar{x} - 25^{th}\\ percentile = 0\n\\]\n\\[\n\\bar{x} - 75^{th}\\ percentile = 1\n\\]\nTherefore, we can say that the data appears to be normally distributed, based on the descriptive statistics alone.\n\n\n\n# Graphical confirmation\nsns.histplot(data = normalized_data_df)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1. Glantz SA. Primer of Biostatistics. 7. ed. McGraw-Hill; 2012.",
    "crumbs": [
      "Lectures",
      "Week 1 - Introduction and Data Summarization"
    ]
  },
  {
    "objectID": "lectures/01 - Introduction and Data Summarization.html#textbook-problems",
    "href": "lectures/01 - Introduction and Data Summarization.html#textbook-problems",
    "title": "Week 1 - Introduction and Data Summarization",
    "section": "",
    "text": "These problems have been taken from the following textbook, which is used in the course:\nGlantz’s “Primer of Biostatistics”1\n\n\n\n# Import data\nrna_levels_seroconversion = [79725, 12862, 18022, 76712, 256440, 14013, 46083, 6808, 85781, 1251, 6081, 50397, 11020, 13633, 1064, 496433, 25308, 6616, 11210, 13900]\n\nrna_levels_sc_df = pd.DataFrame(rna_levels_seroconversion, columns=['RNA_Level'])\n\n\n# Calculate the descriptive statistics \nrna_levels_sc_descriptive = pd.DataFrame(rna_levels_sc_df.describe().round(), dtype=int)\nrna_levels_sc_descriptive\n\n\n\n\n\n\n\n\n\nRNA_Level\n\n\n\n\ncount\n20\n\n\nmean\n61668\n\n\nstd\n117539\n\n\nmin\n1064\n\n\n25%\n9967\n\n\n50%\n13956\n\n\n75%\n56976\n\n\nmax\n496433\n\n\n\n\n\n\n\n\n\n# Determining normality from descriptive stats\n\nsc_mean = rna_levels_sc_descriptive.loc[\"mean\", \"RNA_Level\"]\nsc_median = rna_levels_sc_descriptive.loc[\"50%\", \"RNA_Level\"]\n\nmean_to_25th = sc_mean - rna_levels_sc_descriptive.loc[\"25%\", \"RNA_Level\"]\nmean_to_75th = sc_mean - rna_levels_sc_descriptive.loc[\"75%\", \"RNA_Level\"]\n\n\n\nFrom the descriptive statistics, the data does not appear to be normally distributed. First, there is some skew to the data, as the mean (61668) is far off from the median (13956).\nAdditionally, the difference between the mean and 25th percentile \\(\\neq\\) difference between the mean and 75th percentile:\n\\[\n\\bar{x} - 25^{th}\\ percentile = 51701\n\\]\n\\[\n\\bar{x} - 75^{th}\\ percentile = 4692\n\\]\nTherefore, we can say that the data appears to be not normally distributed, based on the descriptive statistics alone.\n\n\n\n# Graphical confirmation\nsns.histplot(data = rna_levels_seroconversion)\n\n\n\n\n\n\n\n\n\n\n\n\n# Import Data\nnormalized_data = [4.90, 4.11, 4.26, 4.88, 5.41, 4.15, 4.66, 3.83, 4.93, 3.10, 3.78, 4.70, 4.04, 4.13, 3.03, 5.70, 4.40, 3.82, 4.05, 4.14]\n\nnormalized_data_df = pd.DataFrame(normalized_data, columns=['Pain_Score'])\n\n\n# Calculate the descriptive statistics \nnorm_data_descriptive = pd.DataFrame(normalized_data_df.describe().round(), dtype=int)\nnorm_data_descriptive\n\n\n\n\n\n\n\n\n\nPain_Score\n\n\n\n\ncount\n20\n\n\nmean\n4\n\n\nstd\n1\n\n\nmin\n3\n\n\n25%\n4\n\n\n50%\n4\n\n\n75%\n5\n\n\nmax\n6\n\n\n\n\n\n\n\n\n\n# Determining normality from descriptive stats\n\nnorm_mean = norm_data_descriptive.loc[\"mean\", \"Pain_Score\"]\nnorm_median = norm_data_descriptive.loc[\"50%\", \"Pain_Score\"]\n\nmean_to_25th = norm_mean - norm_data_descriptive.loc[\"25%\", \"Pain_Score\"]\nmean_to_75th = norm_data_descriptive.loc[\"75%\", \"Pain_Score\"] - norm_mean\n\n\n\nFrom the descriptive statistics, the data does appear to be normally distributed. First, the mean (4) \\(=\\) the median (4).\nAdditionally, the difference between the mean and 25th percentile \\(\\approx\\) difference between the mean and 75th percentile:\n\\[\n\\bar{x} - 25^{th}\\ percentile = 0\n\\]\n\\[\n\\bar{x} - 75^{th}\\ percentile = 1\n\\]\nTherefore, we can say that the data appears to be normally distributed, based on the descriptive statistics alone.\n\n\n\n# Graphical confirmation\nsns.histplot(data = normalized_data_df)",
    "crumbs": [
      "Lectures",
      "Week 1 - Introduction and Data Summarization"
    ]
  },
  {
    "objectID": "lectures/01 - Introduction and Data Summarization.html#references",
    "href": "lectures/01 - Introduction and Data Summarization.html#references",
    "title": "Week 1 - Introduction and Data Summarization",
    "section": "",
    "text": "1. Glantz SA. Primer of Biostatistics. 7. ed. McGraw-Hill; 2012.",
    "crumbs": [
      "Lectures",
      "Week 1 - Introduction and Data Summarization"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Essentials of Patient Oriented Research (EPOR) Spring 2024",
    "section": "",
    "text": "Theodore Karrison, Ph.D.\nResearch Professor\nDirector, Biostatics Laboratory\nDepartment of Public Health Sciences\nThursdays, March 28 - June 6, 2024\n5:15 - 6:45 pm"
  },
  {
    "objectID": "index.html#course-information",
    "href": "index.html#course-information",
    "title": "Essentials of Patient Oriented Research (EPOR) Spring 2024",
    "section": "",
    "text": "Theodore Karrison, Ph.D.\nResearch Professor\nDirector, Biostatics Laboratory\nDepartment of Public Health Sciences\nThursdays, March 28 - June 6, 2024\n5:15 - 6:45 pm"
  },
  {
    "objectID": "index.html#web-notebook-information",
    "href": "index.html#web-notebook-information",
    "title": "Essentials of Patient Oriented Research (EPOR) Spring 2024",
    "section": "Web notebook information",
    "text": "Web notebook information\nThis is an online notebook generated as a quarto website. The jupyter notebooks are rendered (without execution) by a GitHub Action on a push to the main branch. Notebooks should be executed locally, so that the cell outputs are in the files, prior to committing changes to git."
  },
  {
    "objectID": "lectures/02 - Analysis of Variance.html",
    "href": "lectures/02 - Analysis of Variance.html",
    "title": "Week 2 - Analysis of Variance",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display, Markdown, Math, Latex\n\n\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\n\nfrom scipy import stats\n\nimport patsy\nfrom statsmodels.stats.anova import AnovaRM\nfrom statsmodels.regression.mixed_linear_model import MixedLMResults",
    "crumbs": [
      "Lectures",
      "Week 2 - Analysis of Variance"
    ]
  },
  {
    "objectID": "lectures/02 - Analysis of Variance.html#one-way-anova",
    "href": "lectures/02 - Analysis of Variance.html#one-way-anova",
    "title": "Week 2 - Analysis of Variance",
    "section": "One-way ANOVA",
    "text": "One-way ANOVA\nO6 - Alkylquanine DNA alkyltransferase activity (AGT) and secondary leukemia\n\ncontrols = [\n    9.25, 7, 5.05, 6.9, 3.8, 9.5333, 7, 6.325, 6.05, 9, 4.0333, 8.0333, 7.1667, 5.775, 8.1, 8.7, 6.4, 5.775, 4.5, 3.2, 4.8333, 5, 11, 8.7, 5.875, 6.2, 9.6, 9.6333, 5.6, 10.45, 3.2, 8.4667, 8.3, 8.7, 8.3,\n]\n\naml_de_novo = [11.15, 5.5, 5.2, 14.8, 4.4, 5.6]\n\naml_t = [5.15, 5.1, 1.7, 0.4, 5.5, 4.5, 6.7, 2.9667, 5.9, 3.8333, 5.55]\n\n\n# Find the maximum length among all lists\nmax_length = max(len(controls), len(aml_de_novo), len(aml_t))\n\n# Pad the shorter lists with NaN values to match the maximum length\ncontrols += [np.nan] * (max_length - len(controls))\naml_de_novo += [np.nan] * (max_length - len(aml_de_novo))\naml_t += [np.nan] * (max_length - len(aml_t))\n\n\ndata = {\"Controls\": controls, \"Primary\": aml_de_novo, \"Secondary\": aml_t}\nsl_df = pd.DataFrame(data)\nsl_df\n\n\n\n\n\n\n\n\n\nControls\nPrimary\nSecondary\n\n\n\n\n0\n9.2500\n11.15\n5.1500\n\n\n1\n7.0000\n5.50\n5.1000\n\n\n2\n5.0500\n5.20\n1.7000\n\n\n3\n6.9000\n14.80\n0.4000\n\n\n4\n3.8000\n4.40\n5.5000\n\n\n5\n9.5333\n5.60\n4.5000\n\n\n6\n7.0000\nNaN\n6.7000\n\n\n7\n6.3250\nNaN\n2.9667\n\n\n8\n6.0500\nNaN\n5.9000\n\n\n9\n9.0000\nNaN\n3.8333\n\n\n10\n4.0333\nNaN\n5.5500\n\n\n11\n8.0333\nNaN\nNaN\n\n\n12\n7.1667\nNaN\nNaN\n\n\n13\n5.7750\nNaN\nNaN\n\n\n14\n8.1000\nNaN\nNaN\n\n\n15\n8.7000\nNaN\nNaN\n\n\n16\n6.4000\nNaN\nNaN\n\n\n17\n5.7750\nNaN\nNaN\n\n\n18\n4.5000\nNaN\nNaN\n\n\n19\n3.2000\nNaN\nNaN\n\n\n20\n4.8333\nNaN\nNaN\n\n\n21\n5.0000\nNaN\nNaN\n\n\n22\n11.0000\nNaN\nNaN\n\n\n23\n8.7000\nNaN\nNaN\n\n\n24\n5.8750\nNaN\nNaN\n\n\n25\n6.2000\nNaN\nNaN\n\n\n26\n9.6000\nNaN\nNaN\n\n\n27\n9.6333\nNaN\nNaN\n\n\n28\n5.6000\nNaN\nNaN\n\n\n29\n10.4500\nNaN\nNaN\n\n\n30\n3.2000\nNaN\nNaN\n\n\n31\n8.4667\nNaN\nNaN\n\n\n32\n8.3000\nNaN\nNaN\n\n\n33\n8.7000\nNaN\nNaN\n\n\n34\n8.3000\nNaN\nNaN\n\n\n\n\n\n\n\n\n\nsl_df_melt = pd.melt(sl_df, var_name=\"Group\", value_name=\"AGT\").dropna(axis=0, subset=['AGT'])\nsl_df_melt\n\n\n\n\n\n\n\n\n\nGroup\nAGT\n\n\n\n\n0\nControls\n9.2500\n\n\n1\nControls\n7.0000\n\n\n2\nControls\n5.0500\n\n\n3\nControls\n6.9000\n\n\n4\nControls\n3.8000\n\n\n5\nControls\n9.5333\n\n\n6\nControls\n7.0000\n\n\n7\nControls\n6.3250\n\n\n8\nControls\n6.0500\n\n\n9\nControls\n9.0000\n\n\n10\nControls\n4.0333\n\n\n11\nControls\n8.0333\n\n\n12\nControls\n7.1667\n\n\n13\nControls\n5.7750\n\n\n14\nControls\n8.1000\n\n\n15\nControls\n8.7000\n\n\n16\nControls\n6.4000\n\n\n17\nControls\n5.7750\n\n\n18\nControls\n4.5000\n\n\n19\nControls\n3.2000\n\n\n20\nControls\n4.8333\n\n\n21\nControls\n5.0000\n\n\n22\nControls\n11.0000\n\n\n23\nControls\n8.7000\n\n\n24\nControls\n5.8750\n\n\n25\nControls\n6.2000\n\n\n26\nControls\n9.6000\n\n\n27\nControls\n9.6333\n\n\n28\nControls\n5.6000\n\n\n29\nControls\n10.4500\n\n\n30\nControls\n3.2000\n\n\n31\nControls\n8.4667\n\n\n32\nControls\n8.3000\n\n\n33\nControls\n8.7000\n\n\n34\nControls\n8.3000\n\n\n35\nPrimary\n11.1500\n\n\n36\nPrimary\n5.5000\n\n\n37\nPrimary\n5.2000\n\n\n38\nPrimary\n14.8000\n\n\n39\nPrimary\n4.4000\n\n\n40\nPrimary\n5.6000\n\n\n70\nSecondary\n5.1500\n\n\n71\nSecondary\n5.1000\n\n\n72\nSecondary\n1.7000\n\n\n73\nSecondary\n0.4000\n\n\n74\nSecondary\n5.5000\n\n\n75\nSecondary\n4.5000\n\n\n76\nSecondary\n6.7000\n\n\n77\nSecondary\n2.9667\n\n\n78\nSecondary\n5.9000\n\n\n79\nSecondary\n3.8333\n\n\n80\nSecondary\n5.5500\n\n\n\n\n\n\n\n\n\nsns.catplot(data=sl_df_melt, x=\"Group\", y=\"AGT\", kind=\"box\", aspect=2)\n\n\n\n\n\n\n\n\n\nAssumptions of ANOVA\n\nEach observation is independent of all other observations\nEach sample must be randomly selected from the population (e.g. patients are randomized into different treatment groups)\nData is normally distributed\nVariances of different populations are equal\n\nThe first two points are the most important.\nIf \\(H_{0}\\) is true:\n\\[\\mu_{Controls} = \\mu_{Primary} = \\mu_{Secondary}\\]\n\n# scipy.stats\nstats.f_oneway(\n    sl_df_melt[sl_df_melt[\"Group\"] == \"Controls\"]['AGT'],\n    sl_df_melt[sl_df_melt[\"Group\"] == \"Primary\"]['AGT'],\n    sl_df_melt[sl_df_melt[\"Group\"] == \"Secondary\"]['AGT']\n)\n\nF_onewayResult(statistic=6.447702046398812, pvalue=0.003267180257693189)\n\n\nfrom https://nicoleeic.github.io/Brain_and_Code/2019/09/02/ANOVA_new.html\nSteps to conducting an ANOVA:\n\nGenerate a model that fits our design\nFit our data to the model to get the parameter estimates\nDerive stats from the summary function of the model\n\n\n# 1. Generate a model that fits our design\n# Linear regression\n\nmy_model = smf.ols(formula=\"AGT ~ Group\", data=sl_df_melt)\n\nmy_model\n\n&lt;statsmodels.regression.linear_model.OLS at 0x7f8cc7f8a610&gt;\n\n\n\n# 2. Fit the data to the model\n\nmy_model_fit = my_model.fit()\nmy_model_fit\n\n&lt;statsmodels.regression.linear_model.RegressionResultsWrapper at 0x7f8cc7fb67d0&gt;\n\n\n\n# 3. Summarize the data\n\nmy_model_fit.summary()\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nAGT\nR-squared:\n0.208\n\n\nModel:\nOLS\nAdj. R-squared:\n0.176\n\n\nMethod:\nLeast Squares\nF-statistic:\n6.448\n\n\nDate:\nThu, 11 Apr 2024\nProb (F-statistic):\n0.00327\n\n\nTime:\n19:26:34\nLog-Likelihood:\n-117.01\n\n\nNo. Observations:\n52\nAIC:\n240.0\n\n\nDf Residuals:\n49\nBIC:\n245.9\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n7.0129\n0.400\n17.539\n0.000\n6.209\n7.816\n\n\nGroup[T.Primary]\n0.7621\n1.045\n0.729\n0.469\n-1.338\n2.863\n\n\nGroup[T.Secondary]\n-2.7129\n0.818\n-3.318\n0.002\n-4.356\n-1.070\n\n\n\n\n\n\nOmnibus:\n1.602\nDurbin-Watson:\n2.315\n\n\nProb(Omnibus):\n0.449\nJarque-Bera (JB):\n1.134\n\n\nSkew:\n0.360\nProb(JB):\n0.567\n\n\nKurtosis:\n3.063\nCond. No.\n3.43\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n# ANOVA Table\nanova_table = sm.stats.anova_lm(my_model_fit, typ=2)\n\ndisplay(anova_table)\n\n\n\n\n\n\n\n\n\nsum_sq\ndf\nF\nPR(&gt;F)\n\n\n\n\nGroup\n72.161746\n2.0\n6.447702\n0.003267\n\n\nResidual\n274.200447\n49.0\nNaN\nNaN",
    "crumbs": [
      "Lectures",
      "Week 2 - Analysis of Variance"
    ]
  },
  {
    "objectID": "lectures/02 - Analysis of Variance.html#homework",
    "href": "lectures/02 - Analysis of Variance.html#homework",
    "title": "Week 2 - Analysis of Variance",
    "section": "Homework",
    "text": "Homework\n\nRheumatoid arthritis treatments\nDuration of morning stiffness is measured for each of the three groups:\n\nPlacebo\nDrug A\nDrug B\n\n\nra_placebo = [5.2, 6.8, 5.6, 6.1]\nra_drug_a = [3.7, 2.4, 5.1, 1.8]\nra_drug_b = [4.8, 5.9, 4.0, 4.7]\n\n\nra_data = {\"Placebo\": ra_placebo, \"Drug A\":ra_drug_a, \"Drug B\":ra_drug_b}\n\nra_df = pd.DataFrame(ra_data)\ndisplay(ra_df)\n\nra_df_melt = pd.melt(ra_df, var_name='Treatment', value_name='Stiffness')\n\n\n\n\n\n\n\n\n\nPlacebo\nDrug A\nDrug B\n\n\n\n\n0\n5.2\n3.7\n4.8\n\n\n1\n6.8\n2.4\n5.9\n\n\n2\n5.6\n5.1\n4.0\n\n\n3\n6.1\n1.8\n4.7\n\n\n\n\n\n\n\n\n\nsns.catplot(data=ra_df_melt, x='Treatment', y='Stiffness', kind='box')\n\n\n\n\n\n\n\n\nUsing scipy.stats:\n\nstats.f_oneway(\n    ra_df_melt[ra_df_melt[\"Treatment\"] == \"Placebo\"]['Stiffness'],\n    ra_df_melt[ra_df_melt[\"Treatment\"] == \"Drug A\"]['Stiffness'],\n    ra_df_melt[ra_df_melt[\"Treatment\"] == \"Drug B\"]['Stiffness']\n)\n\nF_onewayResult(statistic=6.705474171164224, pvalue=0.016482391935724025)\n\n\nUsing statsmodels:\n\n# 1. Generate a model that fits our design\n# Linear regression\nmy_model = smf.ols(formula=\"Stiffness ~ Treatment\", data=ra_df_melt)\n\n# 2. Fit the data to the model\nmy_model_fit = my_model.fit()\n\n\n# 3. Summarize the data\n\nmy_model_fit.summary()\n\n/home/pranav/miniforge3/envs/outcomes/lib/python3.11/site-packages/scipy/stats/_stats_py.py:1971: UserWarning: kurtosistest only valid for n&gt;=20 ... continuing anyway, n=12\n  k, _ = kurtosistest(a, axis)\n\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nStiffness\nR-squared:\n0.598\n\n\nModel:\nOLS\nAdj. R-squared:\n0.509\n\n\nMethod:\nLeast Squares\nF-statistic:\n6.705\n\n\nDate:\nThu, 11 Apr 2024\nProb (F-statistic):\n0.0165\n\n\nTime:\n19:26:34\nLog-Likelihood:\n-15.768\n\n\nNo. Observations:\n12\nAIC:\n37.54\n\n\nDf Residuals:\n9\nBIC:\n38.99\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n3.2500\n0.520\n6.252\n0.000\n2.074\n4.426\n\n\nTreatment[T.Drug B]\n1.6000\n0.735\n2.176\n0.058\n-0.063\n3.263\n\n\nTreatment[T.Placebo]\n2.6750\n0.735\n3.639\n0.005\n1.012\n4.338\n\n\n\n\n\n\nOmnibus:\n0.587\nDurbin-Watson:\n3.235\n\n\nProb(Omnibus):\n0.745\nJarque-Bera (JB):\n0.482\n\n\nSkew:\n0.406\nProb(JB):\n0.786\n\n\nKurtosis:\n2.449\nCond. No.\n3.73\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n# ANOVA Table\nanova_table = sm.stats.anova_lm(my_model_fit, typ=2)\n\ndisplay(anova_table)\n\n\n\n\n\n\n\n\n\nsum_sq\ndf\nF\nPR(&gt;F)\n\n\n\n\nTreatment\n14.4950\n2.0\n6.705474\n0.016482\n\n\nResidual\n9.7275\n9.0\nNaN\nNaN",
    "crumbs": [
      "Lectures",
      "Week 2 - Analysis of Variance"
    ]
  }
]