[
  {
    "objectID": "lectures/02 - Analysis of Variance.html",
    "href": "lectures/02 - Analysis of Variance.html",
    "title": "Week 2 - Analysis of Variance",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display, Markdown, Math, Latex\n\n\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\n\nfrom scipy import stats\n\nimport patsy\nfrom statsmodels.stats.anova import AnovaRM\nfrom statsmodels.regression.mixed_linear_model import MixedLMResults",
    "crumbs": [
      "Lectures",
      "Week 2 - Analysis of Variance"
    ]
  },
  {
    "objectID": "lectures/02 - Analysis of Variance.html#one-way-anova",
    "href": "lectures/02 - Analysis of Variance.html#one-way-anova",
    "title": "Week 2 - Analysis of Variance",
    "section": "One-way ANOVA",
    "text": "One-way ANOVA\nO6 - Alkylquanine DNA alkyltransferase activity (AGT) and secondary leukemia\n\ncontrols = [\n    9.25, 7, 5.05, 6.9, 3.8, 9.5333, 7, 6.325, 6.05, 9, 4.0333, 8.0333, 7.1667, 5.775, 8.1, 8.7, 6.4, 5.775, 4.5, 3.2, 4.8333, 5, 11, 8.7, 5.875, 6.2, 9.6, 9.6333, 5.6, 10.45, 3.2, 8.4667, 8.3, 8.7, 8.3,\n]\n\naml_de_novo = [11.15, 5.5, 5.2, 14.8, 4.4, 5.6]\n\naml_t = [5.15, 5.1, 1.7, 0.4, 5.5, 4.5, 6.7, 2.9667, 5.9, 3.8333, 5.55]\n\n\n# Find the maximum length among all lists\nmax_length = max(len(controls), len(aml_de_novo), len(aml_t))\n\n# Pad the shorter lists with NaN values to match the maximum length\ncontrols += [np.nan] * (max_length - len(controls))\naml_de_novo += [np.nan] * (max_length - len(aml_de_novo))\naml_t += [np.nan] * (max_length - len(aml_t))\n\n\ndata = {\"Controls\": controls, \"Primary\": aml_de_novo, \"Secondary\": aml_t}\nsl_df = pd.DataFrame(data)\nsl_df\n\n\n\n\n\n\n\n\n\nControls\nPrimary\nSecondary\n\n\n\n\n0\n9.2500\n11.15\n5.1500\n\n\n1\n7.0000\n5.50\n5.1000\n\n\n2\n5.0500\n5.20\n1.7000\n\n\n3\n6.9000\n14.80\n0.4000\n\n\n4\n3.8000\n4.40\n5.5000\n\n\n5\n9.5333\n5.60\n4.5000\n\n\n6\n7.0000\nNaN\n6.7000\n\n\n7\n6.3250\nNaN\n2.9667\n\n\n8\n6.0500\nNaN\n5.9000\n\n\n9\n9.0000\nNaN\n3.8333\n\n\n10\n4.0333\nNaN\n5.5500\n\n\n11\n8.0333\nNaN\nNaN\n\n\n12\n7.1667\nNaN\nNaN\n\n\n13\n5.7750\nNaN\nNaN\n\n\n14\n8.1000\nNaN\nNaN\n\n\n15\n8.7000\nNaN\nNaN\n\n\n16\n6.4000\nNaN\nNaN\n\n\n17\n5.7750\nNaN\nNaN\n\n\n18\n4.5000\nNaN\nNaN\n\n\n19\n3.2000\nNaN\nNaN\n\n\n20\n4.8333\nNaN\nNaN\n\n\n21\n5.0000\nNaN\nNaN\n\n\n22\n11.0000\nNaN\nNaN\n\n\n23\n8.7000\nNaN\nNaN\n\n\n24\n5.8750\nNaN\nNaN\n\n\n25\n6.2000\nNaN\nNaN\n\n\n26\n9.6000\nNaN\nNaN\n\n\n27\n9.6333\nNaN\nNaN\n\n\n28\n5.6000\nNaN\nNaN\n\n\n29\n10.4500\nNaN\nNaN\n\n\n30\n3.2000\nNaN\nNaN\n\n\n31\n8.4667\nNaN\nNaN\n\n\n32\n8.3000\nNaN\nNaN\n\n\n33\n8.7000\nNaN\nNaN\n\n\n34\n8.3000\nNaN\nNaN\n\n\n\n\n\n\n\n\n\nsl_df_melt = pd.melt(sl_df, var_name=\"Group\", value_name=\"AGT\").dropna(axis=0, subset=['AGT'])\nsl_df_melt\n\n\n\n\n\n\n\n\n\nGroup\nAGT\n\n\n\n\n0\nControls\n9.2500\n\n\n1\nControls\n7.0000\n\n\n2\nControls\n5.0500\n\n\n3\nControls\n6.9000\n\n\n4\nControls\n3.8000\n\n\n5\nControls\n9.5333\n\n\n6\nControls\n7.0000\n\n\n7\nControls\n6.3250\n\n\n8\nControls\n6.0500\n\n\n9\nControls\n9.0000\n\n\n10\nControls\n4.0333\n\n\n11\nControls\n8.0333\n\n\n12\nControls\n7.1667\n\n\n13\nControls\n5.7750\n\n\n14\nControls\n8.1000\n\n\n15\nControls\n8.7000\n\n\n16\nControls\n6.4000\n\n\n17\nControls\n5.7750\n\n\n18\nControls\n4.5000\n\n\n19\nControls\n3.2000\n\n\n20\nControls\n4.8333\n\n\n21\nControls\n5.0000\n\n\n22\nControls\n11.0000\n\n\n23\nControls\n8.7000\n\n\n24\nControls\n5.8750\n\n\n25\nControls\n6.2000\n\n\n26\nControls\n9.6000\n\n\n27\nControls\n9.6333\n\n\n28\nControls\n5.6000\n\n\n29\nControls\n10.4500\n\n\n30\nControls\n3.2000\n\n\n31\nControls\n8.4667\n\n\n32\nControls\n8.3000\n\n\n33\nControls\n8.7000\n\n\n34\nControls\n8.3000\n\n\n35\nPrimary\n11.1500\n\n\n36\nPrimary\n5.5000\n\n\n37\nPrimary\n5.2000\n\n\n38\nPrimary\n14.8000\n\n\n39\nPrimary\n4.4000\n\n\n40\nPrimary\n5.6000\n\n\n70\nSecondary\n5.1500\n\n\n71\nSecondary\n5.1000\n\n\n72\nSecondary\n1.7000\n\n\n73\nSecondary\n0.4000\n\n\n74\nSecondary\n5.5000\n\n\n75\nSecondary\n4.5000\n\n\n76\nSecondary\n6.7000\n\n\n77\nSecondary\n2.9667\n\n\n78\nSecondary\n5.9000\n\n\n79\nSecondary\n3.8333\n\n\n80\nSecondary\n5.5500\n\n\n\n\n\n\n\n\n\nsns.catplot(data=sl_df_melt, x=\"Group\", y=\"AGT\", kind=\"box\", aspect=2)\n\n\n\n\n\n\n\n\n\nAssumptions of ANOVA\n\nEach observation is independent of all other observations\nEach sample must be randomly selected from the population (e.g. patients are randomized into different treatment groups)\nData is normally distributed\nVariances of different populations are equal\n\nThe first two points are the most important.\nIf \\(H_{0}\\) is true:\n\\[\\mu_{Controls} = \\mu_{Primary} = \\mu_{Secondary}\\]\n\n# scipy.stats\nstats.f_oneway(\n    sl_df_melt[sl_df_melt[\"Group\"] == \"Controls\"]['AGT'],\n    sl_df_melt[sl_df_melt[\"Group\"] == \"Primary\"]['AGT'],\n    sl_df_melt[sl_df_melt[\"Group\"] == \"Secondary\"]['AGT']\n)\n\nF_onewayResult(statistic=6.447702046398812, pvalue=0.003267180257693189)\n\n\nfrom https://nicoleeic.github.io/Brain_and_Code/2019/09/02/ANOVA_new.html\nSteps to conducting an ANOVA:\n\nGenerate a model that fits our design\nFit our data to the model to get the parameter estimates\nDerive stats from the summary function of the model\n\n\n# 1. Generate a model that fits our design\n# Linear regression\n\nmy_model = smf.ols(formula=\"AGT ~ Group\", data=sl_df_melt)\n\nmy_model\n\n&lt;statsmodels.regression.linear_model.OLS at 0x7f8cc7f8a610&gt;\n\n\n\n# 2. Fit the data to the model\n\nmy_model_fit = my_model.fit()\nmy_model_fit\n\n&lt;statsmodels.regression.linear_model.RegressionResultsWrapper at 0x7f8cc7fb67d0&gt;\n\n\n\n# 3. Summarize the data\n\nmy_model_fit.summary()\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nAGT\nR-squared:\n0.208\n\n\nModel:\nOLS\nAdj. R-squared:\n0.176\n\n\nMethod:\nLeast Squares\nF-statistic:\n6.448\n\n\nDate:\nThu, 11 Apr 2024\nProb (F-statistic):\n0.00327\n\n\nTime:\n19:26:34\nLog-Likelihood:\n-117.01\n\n\nNo. Observations:\n52\nAIC:\n240.0\n\n\nDf Residuals:\n49\nBIC:\n245.9\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n7.0129\n0.400\n17.539\n0.000\n6.209\n7.816\n\n\nGroup[T.Primary]\n0.7621\n1.045\n0.729\n0.469\n-1.338\n2.863\n\n\nGroup[T.Secondary]\n-2.7129\n0.818\n-3.318\n0.002\n-4.356\n-1.070\n\n\n\n\n\n\nOmnibus:\n1.602\nDurbin-Watson:\n2.315\n\n\nProb(Omnibus):\n0.449\nJarque-Bera (JB):\n1.134\n\n\nSkew:\n0.360\nProb(JB):\n0.567\n\n\nKurtosis:\n3.063\nCond. No.\n3.43\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n# ANOVA Table\nanova_table = sm.stats.anova_lm(my_model_fit, typ=2)\n\ndisplay(anova_table)\n\n\n\n\n\n\n\n\n\nsum_sq\ndf\nF\nPR(&gt;F)\n\n\n\n\nGroup\n72.161746\n2.0\n6.447702\n0.003267\n\n\nResidual\n274.200447\n49.0\nNaN\nNaN",
    "crumbs": [
      "Lectures",
      "Week 2 - Analysis of Variance"
    ]
  },
  {
    "objectID": "lectures/02 - Analysis of Variance.html#homework",
    "href": "lectures/02 - Analysis of Variance.html#homework",
    "title": "Week 2 - Analysis of Variance",
    "section": "Homework",
    "text": "Homework\n\nRheumatoid arthritis treatments\nDuration of morning stiffness is measured for each of the three groups:\n\nPlacebo\nDrug A\nDrug B\n\n\nra_placebo = [5.2, 6.8, 5.6, 6.1]\nra_drug_a = [3.7, 2.4, 5.1, 1.8]\nra_drug_b = [4.8, 5.9, 4.0, 4.7]\n\n\nra_data = {\"Placebo\": ra_placebo, \"Drug A\":ra_drug_a, \"Drug B\":ra_drug_b}\n\nra_df = pd.DataFrame(ra_data)\ndisplay(ra_df)\n\nra_df_melt = pd.melt(ra_df, var_name='Treatment', value_name='Stiffness')\n\n\n\n\n\n\n\n\n\nPlacebo\nDrug A\nDrug B\n\n\n\n\n0\n5.2\n3.7\n4.8\n\n\n1\n6.8\n2.4\n5.9\n\n\n2\n5.6\n5.1\n4.0\n\n\n3\n6.1\n1.8\n4.7\n\n\n\n\n\n\n\n\n\nsns.catplot(data=ra_df_melt, x='Treatment', y='Stiffness', kind='box')\n\n\n\n\n\n\n\n\nUsing scipy.stats:\n\nstats.f_oneway(\n    ra_df_melt[ra_df_melt[\"Treatment\"] == \"Placebo\"]['Stiffness'],\n    ra_df_melt[ra_df_melt[\"Treatment\"] == \"Drug A\"]['Stiffness'],\n    ra_df_melt[ra_df_melt[\"Treatment\"] == \"Drug B\"]['Stiffness']\n)\n\nF_onewayResult(statistic=6.705474171164224, pvalue=0.016482391935724025)\n\n\nUsing statsmodels:\n\n# 1. Generate a model that fits our design\n# Linear regression\nmy_model = smf.ols(formula=\"Stiffness ~ Treatment\", data=ra_df_melt)\n\n# 2. Fit the data to the model\nmy_model_fit = my_model.fit()\n\n\n# 3. Summarize the data\n\nmy_model_fit.summary()\n\n/home/pranav/miniforge3/envs/outcomes/lib/python3.11/site-packages/scipy/stats/_stats_py.py:1971: UserWarning: kurtosistest only valid for n&gt;=20 ... continuing anyway, n=12\n  k, _ = kurtosistest(a, axis)\n\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nStiffness\nR-squared:\n0.598\n\n\nModel:\nOLS\nAdj. R-squared:\n0.509\n\n\nMethod:\nLeast Squares\nF-statistic:\n6.705\n\n\nDate:\nThu, 11 Apr 2024\nProb (F-statistic):\n0.0165\n\n\nTime:\n19:26:34\nLog-Likelihood:\n-15.768\n\n\nNo. Observations:\n12\nAIC:\n37.54\n\n\nDf Residuals:\n9\nBIC:\n38.99\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n3.2500\n0.520\n6.252\n0.000\n2.074\n4.426\n\n\nTreatment[T.Drug B]\n1.6000\n0.735\n2.176\n0.058\n-0.063\n3.263\n\n\nTreatment[T.Placebo]\n2.6750\n0.735\n3.639\n0.005\n1.012\n4.338\n\n\n\n\n\n\nOmnibus:\n0.587\nDurbin-Watson:\n3.235\n\n\nProb(Omnibus):\n0.745\nJarque-Bera (JB):\n0.482\n\n\nSkew:\n0.406\nProb(JB):\n0.786\n\n\nKurtosis:\n2.449\nCond. No.\n3.73\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n# ANOVA Table\nanova_table = sm.stats.anova_lm(my_model_fit, typ=2)\n\ndisplay(anova_table)\n\n\n\n\n\n\n\n\n\nsum_sq\ndf\nF\nPR(&gt;F)\n\n\n\n\nTreatment\n14.4950\n2.0\n6.705474\n0.016482\n\n\nResidual\n9.7275\n9.0\nNaN\nNaN",
    "crumbs": [
      "Lectures",
      "Week 2 - Analysis of Variance"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Essentials of Patient Oriented Research (EPOR) Spring 2024",
    "section": "",
    "text": "Theodore Karrison, Ph.D.\nResearch Professor\nDirector, Biostatics Laboratory\nDepartment of Public Health Sciences\nThursdays, March 28 - June 6, 2024\n5:15 - 6:45 pm"
  },
  {
    "objectID": "index.html#course-information",
    "href": "index.html#course-information",
    "title": "Essentials of Patient Oriented Research (EPOR) Spring 2024",
    "section": "",
    "text": "Theodore Karrison, Ph.D.\nResearch Professor\nDirector, Biostatics Laboratory\nDepartment of Public Health Sciences\nThursdays, March 28 - June 6, 2024\n5:15 - 6:45 pm"
  },
  {
    "objectID": "index.html#web-notebook-information",
    "href": "index.html#web-notebook-information",
    "title": "Essentials of Patient Oriented Research (EPOR) Spring 2024",
    "section": "Web notebook information",
    "text": "Web notebook information\nThis is an online notebook generated as a quarto website. The jupyter notebooks are rendered (without execution) by a GitHub Action on a push to the main branch. Notebooks should be executed locally, so that the cell outputs are in the files, prior to commiting changes to git."
  },
  {
    "objectID": "lectures/01 - Introduction and Data Summarization.html",
    "href": "lectures/01 - Introduction and Data Summarization.html",
    "title": "Week 1 - Introduction and Data Summarization",
    "section": "",
    "text": "import pandas as pd import numpy as np\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns \nfrom IPython.display import display, Markdown, Math, Latex\n\n\n\nThese problems have been taken from the following textbook, which is used in the course:\nGlantz’s “Primer of Biostatistics”1\n\n\n\n# Import data\nrna_levels_seroconversion = [79725, 12862, 18022, 76712, 256440, 14013, 46083, 6808, 85781, 1251, 6081, 50397, 11020, 13633, 1064, 496433, 25308, 6616, 11210, 13900]\n\nrna_levels_sc_df = pd.DataFrame(rna_levels_seroconversion, columns=['RNA_Level'])\n\n\n# Calculate the descriptive statistics \nrna_levels_sc_descriptive = pd.DataFrame(rna_levels_sc_df.describe().round(), dtype=int)\nrna_levels_sc_descriptive\n\n\n\n\n\n\n\n\n\nRNA_Level\n\n\n\n\ncount\n20\n\n\nmean\n61668\n\n\nstd\n117539\n\n\nmin\n1064\n\n\n25%\n9967\n\n\n50%\n13956\n\n\n75%\n56976\n\n\nmax\n496433\n\n\n\n\n\n\n\n\n\n# Determining normality from descriptive stats\n\nsc_mean = rna_levels_sc_descriptive.loc[\"mean\", \"RNA_Level\"]\nsc_median = rna_levels_sc_descriptive.loc[\"50%\", \"RNA_Level\"]\n\nmean_to_25th = sc_mean - rna_levels_sc_descriptive.loc[\"25%\", \"RNA_Level\"]\nmean_to_75th = sc_mean - rna_levels_sc_descriptive.loc[\"75%\", \"RNA_Level\"]\n\n\n\nFrom the descriptive statistics, the data does not appear to be normally distributed. First, there is some skew to the data, as the mean (61668) is far off from the median (13956).\nAdditionally, the difference between the mean and 25th percentile \\(\\neq\\) difference between the mean and 75th percentile:\n\\[\n\\bar{x} - 25^{th}\\ percentile = 51701\n\\]\n\\[\n\\bar{x} - 75^{th}\\ percentile = 4692\n\\]\nTherefore, we can say that the data appears to be not normally distributed, based on the descriptive statistics alone.\n\n\n\n# Graphical confirmation\nsns.histplot(data = rna_levels_seroconversion)\n\n\n\n\n\n\n\n\n\n\n\n\n# Import Data\nnormalized_data = [4.90, 4.11, 4.26, 4.88, 5.41, 4.15, 4.66, 3.83, 4.93, 3.10, 3.78, 4.70, 4.04, 4.13, 3.03, 5.70, 4.40, 3.82, 4.05, 4.14]\n\nnormalized_data_df = pd.DataFrame(normalized_data, columns=['Pain_Score'])\n\n\n# Calculate the descriptive statistics \nnorm_data_descriptive = pd.DataFrame(normalized_data_df.describe().round(), dtype=int)\nnorm_data_descriptive\n\n\n\n\n\n\n\n\n\nPain_Score\n\n\n\n\ncount\n20\n\n\nmean\n4\n\n\nstd\n1\n\n\nmin\n3\n\n\n25%\n4\n\n\n50%\n4\n\n\n75%\n5\n\n\nmax\n6\n\n\n\n\n\n\n\n\n\n# Determining normality from descriptive stats\n\nnorm_mean = norm_data_descriptive.loc[\"mean\", \"Pain_Score\"]\nnorm_median = norm_data_descriptive.loc[\"50%\", \"Pain_Score\"]\n\nmean_to_25th = norm_mean - norm_data_descriptive.loc[\"25%\", \"Pain_Score\"]\nmean_to_75th = norm_data_descriptive.loc[\"75%\", \"Pain_Score\"] - norm_mean\n\n\n\nFrom the descriptive statistics, the data does appear to be normally distributed. First, the mean (4) \\(=\\) the median (4).\nAdditionally, the difference between the mean and 25th percentile \\(\\approx\\) difference between the mean and 75th percentile:\n\\[\n\\bar{x} - 25^{th}\\ percentile = 0\n\\]\n\\[\n\\bar{x} - 75^{th}\\ percentile = 1\n\\]\nTherefore, we can say that the data appears to be normally distributed, based on the descriptive statistics alone.\n\n\n\n# Graphical confirmation\nsns.histplot(data = normalized_data_df)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1. Glantz SA. Primer of Biostatistics. 7. ed. McGraw-Hill; 2012.",
    "crumbs": [
      "Lectures",
      "Week 1 - Introduction and Data Summarization"
    ]
  },
  {
    "objectID": "lectures/01 - Introduction and Data Summarization.html#textbook-problems",
    "href": "lectures/01 - Introduction and Data Summarization.html#textbook-problems",
    "title": "Week 1 - Introduction and Data Summarization",
    "section": "",
    "text": "These problems have been taken from the following textbook, which is used in the course:\nGlantz’s “Primer of Biostatistics”1\n\n\n\n# Import data\nrna_levels_seroconversion = [79725, 12862, 18022, 76712, 256440, 14013, 46083, 6808, 85781, 1251, 6081, 50397, 11020, 13633, 1064, 496433, 25308, 6616, 11210, 13900]\n\nrna_levels_sc_df = pd.DataFrame(rna_levels_seroconversion, columns=['RNA_Level'])\n\n\n# Calculate the descriptive statistics \nrna_levels_sc_descriptive = pd.DataFrame(rna_levels_sc_df.describe().round(), dtype=int)\nrna_levels_sc_descriptive\n\n\n\n\n\n\n\n\n\nRNA_Level\n\n\n\n\ncount\n20\n\n\nmean\n61668\n\n\nstd\n117539\n\n\nmin\n1064\n\n\n25%\n9967\n\n\n50%\n13956\n\n\n75%\n56976\n\n\nmax\n496433\n\n\n\n\n\n\n\n\n\n# Determining normality from descriptive stats\n\nsc_mean = rna_levels_sc_descriptive.loc[\"mean\", \"RNA_Level\"]\nsc_median = rna_levels_sc_descriptive.loc[\"50%\", \"RNA_Level\"]\n\nmean_to_25th = sc_mean - rna_levels_sc_descriptive.loc[\"25%\", \"RNA_Level\"]\nmean_to_75th = sc_mean - rna_levels_sc_descriptive.loc[\"75%\", \"RNA_Level\"]\n\n\n\nFrom the descriptive statistics, the data does not appear to be normally distributed. First, there is some skew to the data, as the mean (61668) is far off from the median (13956).\nAdditionally, the difference between the mean and 25th percentile \\(\\neq\\) difference between the mean and 75th percentile:\n\\[\n\\bar{x} - 25^{th}\\ percentile = 51701\n\\]\n\\[\n\\bar{x} - 75^{th}\\ percentile = 4692\n\\]\nTherefore, we can say that the data appears to be not normally distributed, based on the descriptive statistics alone.\n\n\n\n# Graphical confirmation\nsns.histplot(data = rna_levels_seroconversion)\n\n\n\n\n\n\n\n\n\n\n\n\n# Import Data\nnormalized_data = [4.90, 4.11, 4.26, 4.88, 5.41, 4.15, 4.66, 3.83, 4.93, 3.10, 3.78, 4.70, 4.04, 4.13, 3.03, 5.70, 4.40, 3.82, 4.05, 4.14]\n\nnormalized_data_df = pd.DataFrame(normalized_data, columns=['Pain_Score'])\n\n\n# Calculate the descriptive statistics \nnorm_data_descriptive = pd.DataFrame(normalized_data_df.describe().round(), dtype=int)\nnorm_data_descriptive\n\n\n\n\n\n\n\n\n\nPain_Score\n\n\n\n\ncount\n20\n\n\nmean\n4\n\n\nstd\n1\n\n\nmin\n3\n\n\n25%\n4\n\n\n50%\n4\n\n\n75%\n5\n\n\nmax\n6\n\n\n\n\n\n\n\n\n\n# Determining normality from descriptive stats\n\nnorm_mean = norm_data_descriptive.loc[\"mean\", \"Pain_Score\"]\nnorm_median = norm_data_descriptive.loc[\"50%\", \"Pain_Score\"]\n\nmean_to_25th = norm_mean - norm_data_descriptive.loc[\"25%\", \"Pain_Score\"]\nmean_to_75th = norm_data_descriptive.loc[\"75%\", \"Pain_Score\"] - norm_mean\n\n\n\nFrom the descriptive statistics, the data does appear to be normally distributed. First, the mean (4) \\(=\\) the median (4).\nAdditionally, the difference between the mean and 25th percentile \\(\\approx\\) difference between the mean and 75th percentile:\n\\[\n\\bar{x} - 25^{th}\\ percentile = 0\n\\]\n\\[\n\\bar{x} - 75^{th}\\ percentile = 1\n\\]\nTherefore, we can say that the data appears to be normally distributed, based on the descriptive statistics alone.\n\n\n\n# Graphical confirmation\nsns.histplot(data = normalized_data_df)",
    "crumbs": [
      "Lectures",
      "Week 1 - Introduction and Data Summarization"
    ]
  },
  {
    "objectID": "lectures/01 - Introduction and Data Summarization.html#references",
    "href": "lectures/01 - Introduction and Data Summarization.html#references",
    "title": "Week 1 - Introduction and Data Summarization",
    "section": "",
    "text": "1. Glantz SA. Primer of Biostatistics. 7. ed. McGraw-Hill; 2012.",
    "crumbs": [
      "Lectures",
      "Week 1 - Introduction and Data Summarization"
    ]
  }
]